<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Real-Time Streaming Analytics — FarmSmart Blog — Munyaradzi Comfort Zhou</title>
  <link rel="stylesheet" href="../../css/variables.css">
  <link rel="stylesheet" href="../../css/main.css">
</head>
<body>
  <nav class="nav">
    <div class="nav-inner">
      <a href="../../index.html" class="nav-brand">MCZ</a>
      <button class="nav-toggle" aria-label="Toggle navigation">
        <span></span><span></span><span></span>
      </button>
      <ul class="nav-links">
        <li><a href="../../index.html">Home</a></li>
        <li><a href="../../projects.html">Projects</a></li>
        <li><a href="../../blogs.html">Blog</a></li>
        <li><a href="../../cv.html">CV</a></li>
      </ul>
    </div>
  </nav>
  <main class="container">
    <article>
      <div class="post-header">
        <h1>Building FarmSmart &mdash; Real-Time Streaming Analytics</h1>
        <p class="meta">FarmSmart Series &middot; Part 4 of 6</p>
      </div>
      <div class="post-content">

        <h2>Why Real-Time Processing Matters</h2>

        <p>Imagine you're monitoring a field, and soil moisture drops from 20% to 12% in 30 minutes. That could mean a broken irrigation pipe. By the time you check the data tomorrow morning, the crop might already be stressed.</p>

        <p>That's exactly the gap streaming analytics is meant to close. Instead of waiting for data to be collected and then analysed in a batch job hours later, we analyse it as it arrives. The difference between "here's what happened yesterday" and "here's what's happening right now" is enormous in agriculture.</p>

        <h2>Batch vs Streaming: The Honest Comparison</h2>

        <p>Traditional batch analytics look like this:</p>
        <ol>
          <li>Collect data for a period (an hour, a day, a week)</li>
          <li>Store everything</li>
          <li>Run analysis at the end</li>
          <li>Generate a report</li>
        </ol>

        <p>It's like taking photos throughout the day and developing them all at once in the evening. Fine for some use cases. But if you need to know what's happening right now, you need a live feed — not yesterday's photos.</p>

        <p>Streaming analytics processes data as it arrives. Each event gets handled immediately. You can react to problems as they happen rather than discovering them in a morning review.</p>

        <h2>Why Apache Spark for Streaming?</h2>

        <p>Imagine a conveyor belt at a factory. Data arrives continuously on the belt, and instead of waiting for everything to pile up before processing, you handle each batch as it arrives. That's Spark Structured Streaming. It's built for exactly this pattern — continuous, high-volume data processing.</p>

        <p>I went with Spark for a few specific reasons:</p>

        <p><strong>Handles High Volume</strong> — Millions of events per second is routine for Spark.<br>
        <strong>Fault Tolerant</strong> — If something crashes, it picks up where it left off. No data gets dropped.<br>
        <strong>Scalable</strong> — Can distribute work across multiple machines when a single node isn't enough.<br>
        <strong>Unified API</strong> — The same code structure works for both batch and streaming, which means no separate codebase to maintain.</p>

        <h2>The Streaming Architecture</h2>

        <p>Here's how data flows through the streaming layer: Kafka → Spark Structured Streaming → analytics jobs → alerts and stored results.</p>

        <p>Data comes in from Kafka continuously. Spark reads it, runs the analytics jobs, and pushes results out — to the alert system, back to the database, wherever they need to go. Let's look at each component.</p>

        <h2>Component 1: Reading from Kafka</h2>

        <p>The first step is getting data from Kafka into Spark. Spark's Kafka connector does the heavy lifting — it reads messages and converts them into streaming DataFrames.</p>

        <h3>Understanding DataFrames</h3>

        <p>A DataFrame is like a spreadsheet in memory. Rows are individual sensor readings. Columns are the fields (sensor_id, field_id, metric, value, timestamp). You can filter, group, and aggregate them — just like you would a spreadsheet, but with millions of rows and in real-time.</p>

        <p>Here's what a streaming DataFrame looks like as a snapshot:</p>

<pre><code>+----------+---------+--------------+-------+-------------------+
|sensor_id |field_id |metric        |value  |timestamp          |
+----------+---------+--------------+-------+-------------------+
|sensor_001|Field A  |soil_moisture |18.5   |2026-01-23 10:00:00|
|sensor_002|Field B  |temperature   |22.3   |2026-01-23 10:00:01|
|sensor_001|Field A  |soil_moisture |18.3   |2026-01-23 10:00:30|
+----------+---------+--------------+-------+-------------------+</code></pre>

        <h3>Reading Streams</h3>

<pre><code class="language-python">stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "sensor-readings") \
    .load()</code></pre>

        <p>This creates a streaming DataFrame. Unlike a regular DataFrame, it doesn't have a fixed set of data — it continuously receives new rows as messages arrive in Kafka.</p>

        <h3>Parsing Messages</h3>

        <p>Kafka messages arrive as raw bytes. We parse the JSON payload into structured columns:</p>

<pre><code class="language-python"># Parse JSON from Kafka message value
parsed = stream.select(
    from_json(col("value").cast("string"), schema).alias("data")
)</code></pre>

        <h2>Component 2: Windowed Aggregations</h2>

        <p>One of the most common things you want to do with a data stream is calculate aggregates over time. "What's the average temperature in the last hour?" That's a windowed aggregation.</p>

        <h3>Understanding Windows</h3>

        <p>A window is just a time period — a slice of the timeline. Think of it like a sliding frame moving along a timeline. At any moment, the window captures all data points that fall within that time period.</p>

<pre><code>Timeline:  |----|----|----|----|----|----|
Windows:     [--]  [--]  [--]  [--]  [--]</code></pre>

        <p>As time moves forward, the window slides forward. Each window contains all data within that time span.</p>

        <h3>Tumbling Windows</h3>

        <p>A tumbling window divides time into non-overlapping buckets:</p>

<pre><code>10:00-10:05  | 10:05-10:10  | 10:10-10:15  | 10:15-10:20</code></pre>

        <p>Data from one bucket doesn't spill into the next. This is great for "hourly average" calculations where you want each hour to stand independently.</p>

<pre><code class="language-python"># Group by field and 1-hour windows
hourly_avg = stream \
    .groupBy(
        window("timestamp", "1 hour"),
        "field_id"
    ) \
    .agg(avg("value").alias("avg_value"))</code></pre>

        <h3>Sliding Windows</h3>

        <p>A sliding window overlaps with its neighbours:</p>

<pre><code>10:00-10:05
    10:02-10:07
        10:04-10:09
            10:06-10:11</code></pre>

        <p>Each window includes some data from the previous window. This is better for rolling averages — smoother trends, updated more frequently.</p>

<pre><code class="language-python"># 5-minute window, sliding every 1 minute
rolling_avg = stream \
    .groupBy(
        window("timestamp", "5 minutes", "1 minute"),
        "field_id"
    ) \
    .agg(avg("value").alias("rolling_avg"))</code></pre>

        <h3>Aggregation Functions</h3>

        <p>We support the usual set:</p>

        <p><strong>Average</strong></p>
<pre><code class="language-python">avg("value")</code></pre>

        <p><strong>Sum</strong></p>
<pre><code class="language-python">sum("value")</code></pre>

        <p><strong>Min/Max</strong></p>
<pre><code class="language-python">min("value"), max("value")</code></pre>

        <p><strong>Count</strong></p>
<pre><code class="language-python">count("value")</code></pre>

        <h2>Component 3: Anomaly Detection</h2>

        <p>Anomaly detection identifies unusual patterns. A sudden temperature spike or a rapid moisture drop could mean a real problem, or it could mean a faulty sensor. Either way, you want to know immediately.</p>

        <h3>Why Anomaly Detection Matters</h3>

        <p>The scenarios we're watching for:</p>
        <ul>
          <li>A sensor malfunctions and starts reporting impossible values</li>
          <li>A pipe breaks and water usage suddenly spikes</li>
          <li>An unexpected weather shift causes a rapid temperature drop</li>
          <li>Pests or disease create unusual patterns in crop health metrics</li>
        </ul>

        <p>Catching these early — in minutes rather than hours — is the whole point of streaming over batch.</p>

        <h3>Z-Score Detection</h3>

        <p>Z-score measures how far a value is from the average, in units of standard deviation. Think of it like a bell curve. Most values cluster in the middle. Values way out in the tails are unusual.</p>

        <p>The math is straightforward:</p>
        <ol>
          <li>Calculate the mean and standard deviation of recent values</li>
          <li>For each new value, compute: z = (value - mean) / std_dev</li>
          <li>If z > threshold (typically 3.0), flag as anomaly</li>
        </ol>

<pre><code class="language-python"># Calculate z-score
mean = windowed_data.agg(avg("value")).collect()[0][0]
std = windowed_data.agg(stddev("value")).collect()[0][0]
z_score = abs((value - mean) / std)

if z_score > 3.0:
    flag_as_anomaly()</code></pre>

        <p><strong>Concrete example:</strong></p>
        <ul>
          <li>Recent temperatures: 20°C, 21°C, 20°C, 22°C, 19°C</li>
          <li>Mean: 20.4°C, Std dev: 1.02°C</li>
          <li>New reading: 25°C</li>
          <li>Z-score: (25 - 20.4) / 1.02 = 4.5</li>
          <li>Since 4.5 > 3.0, this is flagged as an anomaly</li>
        </ul>

        <h3>Threshold Detection</h3>

        <p>Sometimes you just know the acceptable range from domain knowledge. Soil moisture should be between 15% and 80%. Anything outside that range is worth flagging, no statistics required:</p>

<pre><code class="language-python">if value < min_threshold or value > max_threshold:
    flag_as_anomaly()</code></pre>

        <p>Simpler than z-score, but it requires knowing your thresholds upfront. For well-understood metrics, this is actually the more reliable approach.</p>

        <h3>EWMA (Exponentially Weighted Moving Average)</h3>

        <p>EWMA gives more weight to recent values. Think of it like a memory that fades — what happened a minute ago matters more than what happened an hour ago:</p>

<pre><code class="language-python">ewma = alpha * current_value + (1 - alpha) * previous_ewma</code></pre>

        <p>Where alpha (between 0 and 1) controls how much weight recent values get. Higher alpha = more responsive to recent changes. If the current value diverges significantly from the EWMA, something unusual is happening.</p>

        <h3>Combining Methods</h3>

        <p>In practice, I use multiple methods together. Z-score catches statistical outliers. Threshold detection catches physically impossible values. EWMA catches gradual drift that neither of the other methods would flag individually. Together, they cover a wider range of failure modes.</p>

        <h2>Component 4: Change-Point Detection</h2>

        <p>Change-point detection is about identifying when a trend shifts — not just when a single reading is unusual, but when the underlying pattern changes. Soil moisture might be stable around 20% for three days, then start a steady decline. That inflection point is important.</p>

        <h3>Why Change Points Matter</h3>

        <p>Change points can signal:</p>
        <ul>
          <li>Equipment failure (readings shift suddenly and stay shifted)</li>
          <li>Environmental changes (weather moves through, conditions reset)</li>
          <li>Crop stress (gradual deterioration in health indicators)</li>
          <li>Irrigation events (moisture jumps when irrigation runs)</li>
        </ul>

        <h3>CUSUM (Cumulative Sum) Method</h3>

        <p>CUSUM accumulates deviations from a target value. If readings consistently drift in one direction — even modestly — the cumulative sum keeps growing until it crosses a threshold. That's your signal that something has changed.</p>

<pre><code class="language-python"># Simplified CUSUM
deviation = value - target
cumulative_sum += deviation

if abs(cumulative_sum) > threshold:
    change_point_detected()</code></pre>

        <p><strong>Concrete example:</strong></p>
        <ul>
          <li>Target soil moisture: 20%</li>
          <li>Readings: 20%, 20%, 19%, 18%, 17%, 16%</li>
          <li>Deviations: 0%, 0%, -1%, -2%, -3%, -4%</li>
          <li>Cumulative sum: -10%</li>
          <li>With threshold at 8%, change point detected at reading 5</li>
        </ul>

        <p>No single reading here was wildly out of range. But the pattern — consistently below target and getting worse — tells you something real is happening.</p>

        <h3>Simple Change Detection</h3>

        <p>For faster signals, just look at consecutive differences:</p>

<pre><code class="language-python">change = current_value - previous_value
if abs(change) > threshold:
    significant_change_detected()</code></pre>

        <p>Less robust to noise than CUSUM, but catches sharp sudden changes immediately.</p>

        <h2>Component 5: Event Correlation</h2>

        <p>Events don't happen in isolation. A soil moisture drop might correlate with irrigation activity being off. A temperature spike might correlate with a weather system. Correlation helps us explain why something happened, not just that it happened.</p>

        <h3>Why Correlation Matters</h3>

        <p>Understanding relationships between events lets us:</p>
        <ul>
          <li>Explain anomalies rather than just flag them</li>
          <li>Reduce false alarms (that temperature spike was a weather event, not a sensor fault)</li>
          <li>Identify root causes rather than just symptoms</li>
          <li>Build better recommendations over time</li>
        </ul>

        <h3>Correlating Sensor Data with Weather</h3>

<pre><code class="language-python"># Join sensor data with weather data
correlated = sensor_stream.join(
    weather_stream,
    on=["field_id", "timestamp"],
    how="inner"
)</code></pre>

        <p>This lets us answer: "Did the temperature spike because of weather, or is there a sensor problem?" If weather data shows the same spike, it's probably weather. If only this one sensor spiked, investigate the sensor.</p>

        <h3>Correlating Anomalies with Events</h3>

<pre><code class="language-python"># Find events near anomaly time
related_events = events.filter(
    (col("timestamp") >= anomaly_time - interval("1 hour")) &
    (col("timestamp") <= anomaly_time + interval("1 hour"))
)</code></pre>

        <h2>Real-World Streaming Jobs</h2>

        <p>Here are the three streaming jobs I built that combine all these techniques into something useful.</p>

        <h3>Drought Detection Job</h3>

        <p>This job continuously monitors soil moisture and flags drought conditions before they become crop losses.</p>

        <p><strong>How it works:</strong></p>
        <ol>
          <li>Calculate average soil moisture over 1-hour windows</li>
          <li>Check if average is below the crop-specific threshold</li>
          <li>If it stays below threshold for more than 2 hours, trigger a drought alert</li>
          <li>Alert includes field location, current reading, and recommended action</li>
        </ol>

        <h3>Irrigation Efficiency Job</h3>

        <p>This job tracks how well the irrigation system is actually working.</p>

        <p><strong>How it works:</strong></p>
        <ol>
          <li>Monitor irrigation events (when and how long systems run)</li>
          <li>Track the resulting change in soil moisture</li>
          <li>Calculate efficiency: (moisture increase) / (water used)</li>
          <li>Alert if efficiency is unusually low — a potential sign of leaks or distribution problems</li>
        </ol>

        <h3>NDVI Stress Detection</h3>

        <p>NDVI (Normalized Difference Vegetation Index) is a measure of plant health derived from satellite or drone imagery. This job detects plant stress early.</p>

        <p><strong>How it works:</strong></p>
        <ol>
          <li>Compare current NDVI to historical baseline for this time of year</li>
          <li>If NDVI drops, check soil moisture at the same location</li>
          <li>Low moisture + low NDVI = likely water stress</li>
          <li>Normal moisture + low NDVI = investigate other causes (disease, pests, nutrients)</li>
        </ol>

        <p>This kind of cross-metric reasoning is where streaming analytics really earns its keep. You're not just looking at one number — you're building a picture from multiple streams simultaneously.</p>

        <h2>Performance and Scalability</h2>

        <h3>Checkpointing</h3>

        <p>Spark periodically saves its current state — which messages have been processed, what the current window state is — so if the system crashes, it can resume from the last checkpoint. Think of it like save points in a video game. You don't lose everything, just whatever happened since the last save.</p>

        <h3>Parallel Processing</h3>

        <p>Spark distributes work across multiple cores or machines:</p>

<pre><code>Data Stream → Split into Partitions → Process in Parallel → Combine Results</code></pre>

        <p>More data? Add more machines. The architecture scales horizontally without changing the code.</p>

        <h3>Backpressure Handling</h3>

        <p>If data arrives faster than we can process it, Spark automatically slows down its consumption rate — it applies backpressure. Data buffers in Kafka (which can hold a lot). Nothing gets dropped. The system catches up when load decreases.</p>

        <h2>Monitoring Streaming Jobs</h2>

        <p>You can't just start a streaming job and walk away. I monitor four key signals:</p>

        <p><strong>Throughput</strong> — How many messages per second are we processing? If this drops suddenly, something's wrong.<br>
        <strong>Latency</strong> — How long between a message arriving and a result being produced? A growing lag means we're falling behind.<br>
        <strong>Backlog</strong> — How many unprocessed messages are sitting in Kafka? A growing backlog means we're not keeping up.<br>
        <strong>Error Rate</strong> — Are processing failures increasing? Even a small uptick deserves investigation.</p>

        <h2>Testing Streaming Systems</h2>

        <p>Testing streaming code is tricky because data is always moving. I use four strategies:</p>

        <p><strong>Unit Tests</strong> — Test individual functions with static sample data. No Kafka, no Spark, just logic.</p>

        <p><strong>Integration Tests</strong> — Spin up a local Kafka and a local Spark instance, run the actual job, verify the output. More setup, but catches real integration issues.</p>

        <p><strong>Performance Tests</strong> — Measure throughput and latency under realistic load. If the job can't keep up in testing, it definitely won't keep up in production.</p>

        <p><strong>Chaos Tests</strong> — Intentionally kill components and verify the system recovers cleanly. Can we kill a Spark worker mid-job and have checkpointing save us? Yes, and it should.</p>

        <h2>What's Next?</h2>

        <p>In Part 5, we get into the conversational engine — the piece that makes all this data actually accessible through a chat interface. We'll cover:</p>
        <ul>
          <li>Building a dialogue system with memory</li>
          <li>Managing conversation context across turns</li>
          <li>Using large language models for natural-sounding responses</li>
          <li>Generating explanations and recommendations that make sense</li>
          <li>Making the whole experience feel like talking to someone who actually knows farming</li>
        </ul>

        <h2>Key Takeaways</h2>

        <ol>
          <li><strong>Streaming is fundamentally different from batch</strong> — You process data as it arrives, which changes everything about how you design analytics.</li>
          <li><strong>Windows enable time-based analysis</strong> — Tumbling and sliding windows let you calculate aggregates over a moving stream of data.</li>
          <li><strong>Use multiple anomaly detection methods</strong> — Z-score, threshold, and EWMA catch different kinds of problems. Don't rely on just one.</li>
          <li><strong>Change-point detection catches what anomaly detection misses</strong> — Gradual drifts and trend shifts are as important as sudden spikes.</li>
          <li><strong>Correlation provides the "why"</strong> — Knowing that something unusual happened is less valuable than understanding why it happened.</li>
          <li><strong>Streaming requires operational care</strong> — Checkpointing, monitoring, and backpressure handling are all essential in production.</li>
        </ol>

        <p>The streaming analytics layer turns raw sensor data into actionable intelligence. It's the difference between knowing what happened yesterday and knowing what's happening right now. In Part 5, we'll see how we make all of this accessible through natural conversation.</p>

      </div>
      <div class="post-nav">
        <a href="part3-nlp.html">&larr; Part 3: NLP &amp; Query Translation</a>
        <a href="part5-conversational.html">Part 5: Conversational Engine &rarr;</a>
      </div>
    </article>
  </main>
  <footer class="footer">
    <div class="container">
      <p>&copy; 2026 Munyaradzi Comfort Zhou. Built with intention.</p>
    </div>
  </footer>
  <script src="../../js/main.js"></script>
</body>
</html>
