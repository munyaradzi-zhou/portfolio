<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Building FarmSmart: Natural Language to Query Translation — FarmSmart Blog — Munyaradzi Comfort Zhou</title>
  <link rel="stylesheet" href="../../css/variables.css">
  <link rel="stylesheet" href="../../css/main.css">
</head>
<body>
  <nav class="nav">
    <div class="nav-inner">
      <a href="../../index.html" class="nav-brand">MCZ</a>
      <button class="nav-toggle" aria-label="Toggle navigation">
        <span></span><span></span><span></span>
      </button>
      <ul class="nav-links">
        <li><a href="../../index.html">Home</a></li>
        <li><a href="../../projects.html">Projects</a></li>
        <li><a href="../../blogs.html">Blog</a></li>
        <li><a href="../../cv.html">CV</a></li>
      </ul>
    </div>
  </nav>
  <main class="container">
    <article>
      <div class="post-header">
        <h1>Building FarmSmart: Natural Language to Query Translation</h1>
        <p class="meta">FarmSmart Series · Part 3 of 6</p>
      </div>
      <div class="post-content">

        <h2>The Translation Challenge</h2>

        <p>Picture a farmer standing in their field, phone in hand, wanting to know: "Which fields have low soil moisture right now?"</p>

        <p>They could open a database tool, learn the table schema, write a SQL query, and figure out how to run it. Or they could just type the question. FarmSmart makes the second option work — and building that translation layer is where most of the interesting engineering lives.</p>

        <p>The system's job is simple to describe and surprisingly tricky to build: take what a farmer types in plain English, figure out what they actually want, and turn it into a real database query. Let's walk through exactly how that works.</p>

        <h2>The Problem: Human Language is Messy</h2>

        <p>The same question can show up in dozens of ways:</p>

        <ul>
          <li>"What's the soil moisture in Field A?"</li>
          <li>"Show me the soil moisture for Field A"</li>
          <li>"How moist is Field A?"</li>
          <li>"Field A soil moisture?"</li>
          <li>"Tell me about the moisture levels in Field A"</li>
        </ul>

        <p>All of these mean the same thing. A computer, though, sees them as completely different strings of text. We need to extract meaning, not just match exact words. That's the core challenge.</p>

        <h2>The Solution: A Multi-Stage Pipeline</h2>

        <p>I break the translation problem into four smaller, manageable stages. Each stage has one job:</p>

        <ol>
          <li>Understand the intent — what does the user want?</li>
          <li>Extract the key details — what specific information matters?</li>
          <li>Build a structured query — how do we actually get the answer?</li>
          <li>Validate the query — is this safe and sensible?</li>
        </ol>

        <p>Let's go through each one.</p>

        <h2>Stage 1: Intent Classification — What Does the User Want?</h2>

        <p>Intent classification is just a fancy way of saying: figure out what someone is actually asking. "Is it hot in Field A?" and "Show me temperature readings for Field A" mean the same thing — the system needs to recognise that.</p>

        <p>Is the user asking for current status? A historical trend? A comparison between fields? An alert condition? These are different questions that need different database queries, so we need to get this right first.</p>

        <h3>Why Machine Learning?</h3>

        <p>You might think simple keyword matching would work:</p>

<pre><code class="language-python">if "trend" in question:
    intent = "trend"
elif "compare" in question:
    intent = "compare"</code></pre>

        <p>And it would — until someone asks "Show me how temperature has changed." Same meaning as "show me the trend", but no keyword match. Human language defeats keyword matching surprisingly quickly. Machine learning learns patterns instead of keywords, so it handles this naturally.</p>

        <h3>Training the Intent Classifier</h3>

        <p>We train the classifier on example questions paired with their correct intents — like teaching by example:</p>

<pre><code>Question: "What's the soil moisture in Field A?"
Intent: query_status

Question: "Show me the temperature trend"
Intent: trend

Question: "Compare soil moisture across fields"
Intent: compare</code></pre>

        <p>After seeing enough examples, the model learns that questions starting with "What's" or "Show me current" often want status, while questions with "trend", "changed", or "over the last week" want historical data. It's pattern recognition, not memorisation.</p>

        <h3>How It Works: TF-IDF and Logistic Regression</h3>

        <p>I use TF-IDF combined with Logistic Regression. Here's what that actually means:</p>

        <p><strong>TF-IDF</strong> converts text into numbers that capture word importance. Common words like "the" and "is" get low scores — they appear everywhere and tell you nothing. Important words like "soil_moisture" or "trend" get high scores because they're specific and meaningful.</p>

        <p><strong>Logistic Regression</strong> is a simple but effective algorithm that learns to classify based on those numerical features. It's not glamorous, but it's fast, interpretable, and works really well for this kind of structured classification task.</p>

        <p>The model outputs both an intent and a confidence score. 0.95 means it's 95% sure. Below 0.7 and we flag the question as ambiguous and ask the user to clarify.</p>

        <h3>Intent Types We Support</h3>

        <p><strong>query_status</strong> — Get current value</p>
        <ul>
          <li>"What's the temperature in Field A?"</li>
          <li>"Show me the soil moisture"</li>
        </ul>

        <p><strong>trend</strong> — Get historical trend</p>
        <ul>
          <li>"How has temperature changed?"</li>
          <li>"Show me the soil moisture trend over the last week"</li>
        </ul>

        <p><strong>compare</strong> — Compare multiple fields or metrics</p>
        <ul>
          <li>"Compare soil moisture in Field A and Field B"</li>
          <li>"Which field has higher temperature?"</li>
        </ul>

        <p><strong>alert</strong> — Check alert conditions</p>
        <ul>
          <li>"Alert me if temperature exceeds 35°C"</li>
          <li>"Which fields have low soil moisture?"</li>
        </ul>

        <h2>Stage 2: Slot Extraction — Finding the Important Details</h2>

        <p>Once we know the intent, we need the specifics. These are called "slots" — the key pieces of information needed to actually construct the query.</p>

        <p>Think of it like filling out a form. The intent tells you which form to use. The slots are the individual fields you need to fill in.</p>

        <h3>What Are Slots?</h3>

        <p>For "What's the soil moisture in Field A?", the slots are:</p>
        <ul>
          <li><strong>metric</strong>: "soil_moisture"</li>
          <li><strong>field</strong>: "Field A"</li>
        </ul>

        <p>For "Show me the temperature trend for Field B over the last 24 hours":</p>
        <ul>
          <li><strong>metric</strong>: "temperature"</li>
          <li><strong>field</strong>: "Field B"</li>
          <li><strong>time_window</strong>: "24 hours"</li>
        </ul>

        <h3>Slot Extraction Methods</h3>

        <p>Here I use regex patterns and heuristics rather than machine learning. Why? Because slots are more structured than intents. We're looking for specific patterns — field names, metric types, time expressions. Regex is fast, predictable, and easy to debug for this kind of work.</p>

        <p><strong>Metric Extraction</strong></p>

<pre><code class="language-python">patterns = {
    "soil_moisture": r"(soil\s+)?moisture|moisture\s+level",
    "temperature": r"temp(erature)?|heat",
    "humidity": r"humidity|moisture\s+in\s+air"
}</code></pre>

        <p><strong>Field Extraction</strong></p>

<pre><code class="language-python"># Matches: "Field A", "field A", "Field-A", etc.
pattern = r"field\s+([A-Z]|\d+)"</code></pre>

        <p><strong>Time Window Extraction</strong></p>

<pre><code class="language-python">patterns = {
    "1h": r"last\s+hour|past\s+hour|1\s+hour",
    "24h": r"last\s+24\s+hours|past\s+day|yesterday",
    "7d": r"last\s+week|past\s+7\s+days|7\s+days"
}</code></pre>

        <h3>Handling Ambiguity</h3>

        <p>Sometimes questions are incomplete. "What's the temperature?" is missing the field. We have two strategies depending on context:</p>
        <ol>
          <li>If the user recently asked about Field A, assume Field A and continue</li>
          <li>If there's no useful context, ask for clarification</li>
        </ol>

        <p>The key insight is that follow-up questions in a conversation are almost always about the same thing the user just asked about. Using context for these cases feels natural and saves a lot of back-and-forth.</p>

        <h2>Stage 3: Query Building — From Slots to Database Query</h2>

        <p>Now we have an intent and a set of slots. We need to turn those into an actual database query.</p>

        <h3>Query Templates</h3>

        <p>I use templates — fill-in-the-blank queries for each intent type. For <code>query_status</code>:</p>

<pre><code class="language-sql">SELECT value, timestamp
FROM sensor_data
WHERE field_id = {field}
  AND metric = {metric}
ORDER BY timestamp DESC
LIMIT 1</code></pre>

        <p>Fill in the slots and you get:</p>

<pre><code class="language-sql">SELECT value, timestamp
FROM sensor_data
WHERE field_id = 'Field A'
  AND metric = 'soil_moisture'
ORDER BY timestamp DESC
LIMIT 1</code></pre>

        <h3>Handling Different Intents</h3>

        <p>Each intent needs a structurally different query:</p>

        <p><strong>query_status</strong> — Simple lookup</p>

<pre><code class="language-sql">SELECT value FROM sensor_data
WHERE field_id = ? AND metric = ?
ORDER BY timestamp DESC LIMIT 1</code></pre>

        <p><strong>trend</strong> — Time series with aggregation</p>

<pre><code class="language-sql">SELECT AVG(value) as avg_value,
       time_bucket('1 hour', timestamp) as hour
FROM sensor_data
WHERE field_id = ? AND metric = ?
  AND timestamp > now() - INTERVAL '24 hours'
GROUP BY hour
ORDER BY hour</code></pre>

        <p><strong>compare</strong> — Multiple fields</p>

<pre><code class="language-sql">SELECT field_id, AVG(value) as avg_value
FROM sensor_data
WHERE field_id IN (?, ?) AND metric = ?
GROUP BY field_id</code></pre>

        <p><strong>alert</strong> — Threshold checking</p>

<pre><code class="language-sql">SELECT field_id, value, timestamp
FROM sensor_data
WHERE metric = ?
  AND value {operator} {threshold}
  AND timestamp > now() - INTERVAL '1 hour'</code></pre>

        <h3>Time Window Conversion</h3>

        <p>One tricky piece is converting natural language time expressions into database-friendly intervals:</p>

<pre><code>"last hour" → timestamp > now() - INTERVAL '1 hour'
"last 24 hours" → timestamp > now() - INTERVAL '24 hours'
"last week" → timestamp > now() - INTERVAL '7 days'
"today" → timestamp >= CURRENT_DATE</code></pre>

<pre><code class="language-python">def time_window_to_interval(time_window: str) -> str:
    """Convert '24h' to 'INTERVAL 24 hours'"""
    if time_window == "1h":
        return "INTERVAL '1 hour'"
    elif time_window == "24h":
        return "INTERVAL '24 hours'"
    # ... more conversions</code></pre>

        <h3>Parameterized Queries</h3>

        <p>I always use parameterized queries instead of string concatenation. This is non-negotiable from a security standpoint.</p>

        <p><strong>Bad (vulnerable to SQL injection):</strong></p>

<pre><code class="language-python">query = f"SELECT * FROM sensor_data WHERE field_id = '{field}'"
# If field = "Field A'; DROP TABLE sensor_data; --"
# This becomes dangerous SQL!</code></pre>

        <p><strong>Good (safe):</strong></p>

<pre><code class="language-python">query = "SELECT * FROM sensor_data WHERE field_id = ?"
params = [field]
# Database handles escaping automatically</code></pre>

        <p>The database driver handles the escaping. You can't accidentally create a SQL injection vulnerability when the user input never gets concatenated into the query string.</p>

        <h2>Stage 4: Query Validation — Safety First</h2>

        <p>Before any query executes, it gets validated. Three concerns: safety, correctness, and performance.</p>

        <h3>Safety Checks</h3>

        <p>We only use parameterized queries (covered above). We also validate that field names and metrics are on our allowed list — if someone tries to inject a field name that doesn't exist, we catch it here, not at query time.</p>

        <h3>Correctness Checks</h3>

        <p>We verify that:</p>
        <ul>
          <li>The referenced field actually exists in our database</li>
          <li>The requested metric is one we support</li>
          <li>The time window is reasonable (not "last 100 years")</li>
          <li>All required parameters are present</li>
        </ul>

        <h3>Performance Checks</h3>

        <p>We enforce some limits to prevent runaway queries:</p>
        <ul>
          <li>Maximum time window: 90 days (prevents full-table scans)</li>
          <li>Maximum fields in a comparison: 10 (keeps queries manageable)</li>
        </ul>

        <p>These limits aren't about being restrictive — they're about keeping the system fast for everyone.</p>

        <h2>Putting It All Together: A Complete Example</h2>

        <p>Let's trace "What's the soil moisture in Field A?" through every stage.</p>

        <p><strong>Step 1: User asks a question</strong></p>

<pre><code>"What's the soil moisture in Field A?"</code></pre>

        <p><strong>Step 2: Intent Classification</strong></p>

<pre><code class="language-python">intent, confidence = classifier.predict(question)
# Result: intent = "query_status", confidence = 0.92</code></pre>

        <p><strong>Step 3: Slot Extraction</strong></p>

<pre><code class="language-python">slots = extractor.extract(question)
# Result: {
#     "metric": "soil_moisture",
#     "field": "Field A"
# }</code></pre>

        <p><strong>Step 4: Query Building</strong></p>

<pre><code class="language-python">query_structure = {
    "intent": "query_status",
    "metric": "soil_moisture",
    "field": "Field A"
}
query = builder.build_query(query_structure)
# Result: SQL query with parameters</code></pre>

        <p><strong>Step 5: Query Validation</strong></p>

<pre><code class="language-python">is_valid = validator.validate(query_structure)
# Checks:
# - Field "Field A" exists? ✓
# - Metric "soil_moisture" is supported? ✓
# - Query is safe? ✓
# Result: Valid</code></pre>

        <p><strong>Step 6: Execute Query</strong></p>

<pre><code class="language-python">result = execute_query(query)
# Result: {"value": 18.5, "timestamp": "2026-01-23T10:00:00Z"}</code></pre>

        <p><strong>Step 7: Format Response</strong></p>

<pre><code>"The soil moisture in Field A is 18.5% (measured at 10:00 AM)"</code></pre>

        <h2>Handling Complex Questions</h2>

        <p>Not every question is a clean, single-slot query. Here are some trickier cases and how we handle them.</p>

        <h3>Example 1: Implicit Information</h3>

        <p><strong>Question:</strong> "How has it changed?"</p>

        <p>Missing metric, missing field. But if the user just asked about soil moisture in Field A, we use that context and interpret this as "How has soil moisture in Field A changed?" This is the most natural way for a human to continue a conversation, so we support it.</p>

        <h3>Example 2: Multiple Metrics</h3>

        <p><strong>Question:</strong> "Compare temperature and humidity in Field A"</p>

        <p>This requires two separate queries — one for temperature, one for humidity — then combining the results before responding. Not complex, just a bit more work in the query builder.</p>

        <h3>Example 3: Relative Time</h3>

        <p><strong>Question:</strong> "Show me data from yesterday"</p>

        <p>We convert "yesterday" to an actual date range based on the current date at query time. Simple but easy to get wrong if you're not careful about timezones.</p>

        <h3>Example 4: Aggregations</h3>

        <p><strong>Question:</strong> "What's the average soil moisture this week?"</p>

        <p>This needs a time window of 7 days, an average aggregation, and the metric soil_moisture. The slot extractor pulls all three, and the query builder generates a GROUP BY query. Here's the fun part — once the pipeline is built correctly, complex questions like this just work.</p>

        <h2>Confidence Scoring</h2>

        <p>Not all translations are equally reliable. I track confidence at multiple stages and combine them into an overall score:</p>

<pre><code class="language-python">overall_confidence = (
    intent_confidence * 0.4 +  # Intent is most important
    slot_confidence * 0.4 +     # Slots are also critical
    validation_score * 0.2      # Validation adds certainty
)</code></pre>

        <p>I use these scores to decide whether to ask for clarification (low confidence), add a warning to the response (medium confidence), or proceed with full confidence. Knowing how sure you are is almost as valuable as the answer itself.</p>

        <h2>Training and Improving the Model</h2>

        <p>The intent classifier improves over time as we gather more data.</p>

        <h3>Training Data</h3>

        <p>I use a dataset of 200 example questions with their correct intents and slots. It's deliberately varied — different phrasings of the same question, edge cases, ambiguous examples. The more diverse the training data, the more robust the model.</p>

        <h3>Evaluation</h3>

        <p>I measure three things:</p>
        <ul>
          <li><strong>Intent Accuracy</strong> — What fraction of intents are classified correctly?</li>
          <li><strong>Slot Extraction Accuracy</strong> — What fraction of slots are extracted correctly?</li>
          <li><strong>End-to-End Accuracy</strong> — What fraction of questions result in the correct final answer?</li>
        </ul>

        <h3>Continuous Improvement</h3>

        <p>When the system gets something wrong, I log the question and the incorrect translation, add it to the training set with the correct label, and retrain periodically. This kind of feedback loop is what makes the system better over time rather than stagnating.</p>

        <h2>Error Handling</h2>

        <p>When translation fails, we fail gracefully.</p>

        <h3>Missing Information</h3>

<pre><code class="language-python">if not field:
    return {
        "error": "missing_field",
        "message": "Which field would you like to query?",
        "confidence": 0.0
    }</code></pre>

        <h3>Ambiguous Questions</h3>

<pre><code class="language-python">if intent_confidence < 0.7:
    return {
        "error": "ambiguous_intent",
        "message": "Could you rephrase your question?",
        "suggestions": ["What's the current value?", "Show me a trend?"]
    }</code></pre>

        <h3>Invalid Combinations</h3>

<pre><code class="language-python">if metric == "soil_moisture" and time_window == "1 minute":
    return {
        "error": "invalid_combination",
        "message": "Soil moisture doesn't change that quickly. Try a longer time window."
    }</code></pre>

        <p>That last one I find particularly satisfying — the system is domain-aware enough to recognise that soil moisture doesn't meaningfully change in 60 seconds and tells the user to ask a more sensible question.</p>

        <h2>Performance Considerations</h2>

        <p>Users expect answers in under 2 seconds. The translation pipeline needs to be fast to leave budget for query execution and response generation.</p>

        <p><strong>Caching</strong> — Common questions and their classifications are cached. "What's the temperature?" doesn't need a fresh model inference every single time.</p>

        <p><strong>Pre-compiled Patterns</strong> — Regex patterns are compiled once at startup, not on every extraction request.</p>

        <p><strong>Model Optimization</strong> — The intent classifier is deliberately lightweight. TF-IDF + Logistic Regression is orders of magnitude faster than a neural network, with comparable accuracy for this kind of structured domain.</p>

        <p><strong>Parallel Processing</strong> — Intent classification and slot extraction are independent, so they can run simultaneously.</p>

        <h2>Testing the Translator</h2>

        <p>I test the translation pipeline at three levels:</p>

        <p><strong>Unit Tests</strong> — Does intent classification work for known examples? Does slot extraction find the right information? Does query building produce correct SQL?</p>

        <p><strong>Integration Tests</strong> — Can we translate a question end-to-end and get a valid result from the database?</p>

        <p><strong>Accuracy Tests</strong> — I hold out a set of examples from training and test the model on those. This gives an honest measurement of real-world performance, not just performance on examples the model has already seen.</p>

        <h2>What's Next?</h2>

        <p>In Part 4, we get into real-time stream processing — how we analyse data as it arrives, detect anomalies, and spot trends in live sensor streams. That's where we go from answering questions about what's happened to catching problems as they unfold. We'll cover:</p>
        <ul>
          <li>Spark Structured Streaming for real-time processing</li>
          <li>Windowed aggregations (how do you calculate a rolling average over a stream?)</li>
          <li>Anomaly detection (how do you spot unusual patterns?)</li>
          <li>Change-point detection (how do you know when something has shifted?)</li>
          <li>Event correlation (how do you connect related events?)</li>
        </ul>

        <h2>Key Takeaways</h2>

        <ol>
          <li><strong>Translation is multi-stage</strong> — Intent, then slots, then query building, then validation. Each stage has a clear job.</li>
          <li><strong>Machine learning for intent, regex for slots</strong> — Use the right tool for each sub-problem. ML handles fuzzy pattern matching; regex handles structured extraction.</li>
          <li><strong>Safety is non-negotiable</strong> — Parameterized queries, allowed-list validation, and query limits prevent both attacks and accidents.</li>
          <li><strong>Confidence matters</strong> — Knowing how sure you are lets you handle ambiguity gracefully instead of silently returning bad answers.</li>
          <li><strong>Context makes conversations work</strong> — Follow-up questions are only natural if the system remembers what came before.</li>
          <li><strong>Fail gracefully</strong> — When something doesn't work, tell the user why and suggest how to fix it.</li>
        </ol>

        <p>The translation system handles a wide variety of questions while staying safe and accurate. It's not perfect — no NLP system is — but it's robust enough for real-world use and gets better over time as we feed it more training data.</p>

      </div>
      <div class="post-nav">
        <a href="part2-data-layer.html">&larr; Part 2: The Data Layer and Ingestion Pipeline</a>
        <a href="part4-streaming.html">Part 4: Real-Time Stream Processing &rarr;</a>
      </div>
    </article>
  </main>
  <footer class="footer">
    <div class="container">
      <p>&copy; 2026 Munyaradzi Comfort Zhou. Built with intention.</p>
    </div>
  </footer>
  <script src="../../js/main.js"></script>
</body>
</html>
