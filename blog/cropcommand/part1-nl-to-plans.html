<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>From Natural Language to Robot Task Plans — CropCommand Blog — Munyaradzi Comfort Zhou</title>
  <link rel="stylesheet" href="../../css/variables.css">
  <link rel="stylesheet" href="../../css/main.css">
</head>
<body>
  <nav class="nav">
    <div class="nav-inner">
      <a href="../../index.html" class="nav-brand">MCZ</a>
      <button class="nav-toggle" aria-label="Toggle navigation">
        <span></span><span></span><span></span>
      </button>
      <ul class="nav-links">
        <li><a href="../../index.html">Home</a></li>
        <li><a href="../../projects.html">Projects</a></li>
        <li><a href="../../blogs.html">Blog</a></li>
        <li><a href="../../cv.html">CV</a></li>
      </ul>
    </div>
  </nav>
  <main class="container">
    <article>
      <div class="post-header">
        <h1>From Natural Language to Robot Task Plans</h1>
        <p class="meta">CropCommand Series · Part 1 of 3</p>
      </div>
      <div class="post-content">

        <p>Picture this: you're standing at the edge of a five-hectare wheat field, a storm front rolling in on the horizon, and you tell your fleet of agricultural robots: <em>"Spray the northern tomato field before the rain, avoiding the organic section."</em></p>

        <p>That one sentence packs a lot in. There's an action (spray), a target (northern tomato field), a timing constraint (before the rain), and a spatial restriction (avoid the organic section). Any farm manager would understand it instantly. For a robot, though, it's a genuinely hard problem — sitting right at the intersection of natural language understanding, task planning, and constraint satisfaction.</p>

        <p>CropCommand is the system I built to bridge that gap. It takes a free-form command from a farmer, parses out the intent, breaks that intent into an ordered sequence of atomic robot actions, and validates the resulting plan against real-world constraints before putting it in front of a human for approval. This post — first in a three-part series — covers the front half of that pipeline: parsing, entity extraction, and task decomposition.</p>

        <h2>Why Natural Language Matters for Agricultural Robotics</h2>

        <p>Modern precision agriculture already uses autonomous platforms — ground sprayers, drone surveyors, robotic harvesters. But programming these machines still means using vendor GUIs, waypoint editors, or scripting languages. The farmer's mental model of "spray the north field before it rains" has to be hand-translated into coordinates, chemical rates, nozzle settings, and timing windows.</p>

        <p>That translation step is the real problem. Most farmers aren't programmers. And even if they were, sitting down to write a task script while a frost warning is in effect is not realistic. You need something that just... understands you.</p>

        <p>CropCommand removes that bottleneck by accepting instructions in the language farmers already use.</p>

        <h2>The Intent Parser: From Words to Structure</h2>

        <p>Every command enters through the <code>IntentParser</code> class, defined in <code>cropcommand.nlp.intent_parser</code>. Its job is to take a raw string and produce a <code>ParsedCommand</code> — a Pydantic model that captures the structured meaning of whatever the farmer just said.</p>

        <h3>Command Intents</h3>

        <p>The parser recognizes eight high-level intent categories, represented by the <code>CommandIntent</code> enum:</p>

<pre><code>class CommandIntent(str, Enum):
    SPRAY = "spray"
    HARVEST = "harvest"
    WEED = "weed"
    SURVEY = "survey"
    NAVIGATE = "navigate"
    SCHEDULE = "schedule"
    STATUS_CHECK = "status_check"
    CANCEL = "cancel"</code></pre>

        <p>These cover the core operations of a diversified farm: applying chemicals, collecting crops, removing weeds, aerial surveying, moving robots, scheduling future work, checking system status, and aborting active tasks.</p>

        <p>Intent detection uses a priority-ordered list of compiled regular expressions:</p>

<pre><code>_INTENT_PATTERNS: List[tuple[CommandIntent, re.Pattern[str]]] = [
    (CommandIntent.CANCEL,       re.compile(r"\b(cancel|abort|stop)\b", re.IGNORECASE)),
    (CommandIntent.STATUS_CHECK, re.compile(r"\b(status|check|report|where is)\b", re.IGNORECASE)),
    (CommandIntent.SPRAY,        re.compile(r"\b(spray|apply|fungicide|herbicide|...)\b", re.IGNORECASE)),
    (CommandIntent.HARVEST,      re.compile(r"\b(harvest|reap|collect|pick|gather)\b", re.IGNORECASE)),
    # ... and so on
]</code></pre>

        <p>The ordering is deliberate. Safety-critical intents like <code>CANCEL</code> are checked first, so "cancel the spray operation" is correctly classified as a cancellation and not a spray command. If nothing matches, the parser falls back to <code>NAVIGATE</code> with a confidence of 0.40, signalling to downstream components that the interpretation is shaky.</p>

        <h3>The ParsedCommand Model</h3>

        <p>The output of parsing is a <code>ParsedCommand</code> instance:</p>

<pre><code>class ParsedCommand(BaseModel):
    intent: CommandIntent
    target_field: Optional[str]
    robot_preference: Optional[str]
    temporal_constraint: Dict[str, Any]
    spatial_constraint: Dict[str, Any]
    parameters: Dict[str, Any]
    raw_command: str
    confidence: float</code></pre>

        <p>Every field earns its place. <code>target_field</code> is the field the command is actually about — not all fields near the command, just the primary one. <code>robot_preference</code> captures explicit robot mentions ("use the drone"). <code>temporal_constraint</code> and <code>spatial_constraint</code> hold structured dictionaries that the downstream planner and constraint engine will consume. <code>parameters</code> is a catch-all for action-specific details: chemical type, application rate, traversal pattern.</p>

        <p>The <code>confidence</code> score is a simple but important signal. When the parser isn't sure — say the command is ambiguous or uses vocabulary the patterns don't cover — that low confidence travels through the pipeline and eventually surfaces to the user as a request for clarification.</p>

        <h2>Entity Extraction: Mining the Details</h2>

        <p>Intent detection answers "what does the user want to do?" Entity extraction answers "where, when, how, and with what?" The <code>entity_extractor</code> module provides five focused extraction functions, each targeting a different entity class.</p>

        <h3>Field References</h3>

<pre><code>_FIELD_PATTERN = re.compile(
    r"\b(?:field\s+)?(north|south|east|west|organic|main|central|upper|lower)"
    r"(?:\s+field)?\b",
    re.IGNORECASE,
)</code></pre>

        <p>The <code>extract_field_references</code> function handles both natural phrasing ("the northern field") and identifier-style references ("field_01"). It normalizes everything into a consistent title-case format like "Field North" — matching the field names used in the farm environment model.</p>

        <h3>Temporal Constraints</h3>

        <p>Temporal extraction is richer. The system recognizes three patterns:</p>

        <ul>
          <li><strong>Absolute deadlines:</strong> "before 3pm", "by 12:00"</li>
          <li><strong>Relative time references:</strong> "tomorrow", "in 2 hours", "this afternoon"</li>
          <li><strong>Time-of-day markers:</strong> "morning", "dawn", "sunset"</li>
        </ul>

        <p>These get extracted into a dictionary:</p>

<pre><code>{
    "deadline": "3pm",
    "deadline_type": "before_time",
    "time_of_day": "afternoon"
}</code></pre>

        <p>This structured representation is what lets the constraint engine later reason about whether a plan can actually finish within the specified window.</p>

        <h3>Spatial Constraints</h3>

        <p>Spatial extraction targets two categories — zones to avoid and zones to focus on:</p>

<pre><code>_AVOID_PATTERN = re.compile(
    r"\b(?:avoid|skip|stay away from|exclude|...)\s+(?:the\s+)?([\w\s]+?)(?:\s+zone|...)\b",
    re.IGNORECASE,
)
_TARGET_ZONE_PATTERN = re.compile(
    r"\b(?:only|just|focus on|concentrate on|target)\s+(?:the\s+)?([\w\s]+?)(?:\s+zone|...)\b",
    re.IGNORECASE,
)</code></pre>

        <p>When the farmer says "avoiding the organic section," the avoid pattern fires and produces <code>{"avoid_zones": ["organic"]}</code>. That spatial restriction gets passed through the entire pipeline and enforced during route planning.</p>

        <h3>Crop and Robot References</h3>

        <p>The extractor also maintains a vocabulary of 30+ common crop types (wheat, corn, tomatoes, strawberries, etc.) and robot keywords (drone, sprayer, harvester, tractor). These are matched with simple word-boundary patterns and fed into the <code>parameters</code> and <code>robot_preference</code> fields of the <code>ParsedCommand</code>.</p>

        <h2>Task Decomposition: From Intent to Atomic Actions</h2>

        <p>Parsing tells us <em>what</em> to do. Decomposition tells us <em>how</em>. Think of it like writing a recipe before you start cooking — you figure out all the steps before you touch anything. Here, we're breaking a high-level intent into an ordered sequence of atomic <code>TaskAction</code> objects that a robot can execute one at a time.</p>

        <h3>The TaskAction Model</h3>

<pre><code>class TaskAction(BaseModel):
    id: int                           # Unique step ID (1-based)
    action_type: ActionType           # NAVIGATE, SPRAY, HARVEST, WEED, SURVEY, CONFIGURE, RETURN_BASE, WAIT
    target_field: Optional[str]       # Which field this step targets
    parameters: Dict[str, Any]        # Step-specific parameters
    estimated_duration_minutes: float  # Time estimate
    dependencies: List[int]           # IDs of prerequisite steps</code></pre>

        <p>The <code>dependencies</code> list is where things get interesting. It encodes the partial ordering between steps — step 4 (spray) can't start until step 3 (navigate) is complete. Like knowing you can't put the roof up before the walls are done. This gives the scheduler flexibility to parallelize independent branches while respecting the causal relationships that actually matter.</p>

        <h3>Intent-Specific Decomposition</h3>

        <p>The <code>TaskDecomposer</code> uses a dispatch table to route each intent to a specialized decomposition function:</p>

<pre><code>_DECOMPOSITION_MAP = {
    CommandIntent.SPRAY:        _decompose_spray,
    CommandIntent.HARVEST:      _decompose_harvest,
    CommandIntent.WEED:         _decompose_weed,
    CommandIntent.SURVEY:       _decompose_survey,
    CommandIntent.NAVIGATE:     _decompose_navigate,
    CommandIntent.STATUS_CHECK: _decompose_status_check,
    CommandIntent.CANCEL:       _decompose_cancel,
}</code></pre>

        <p>Each function encodes domain knowledge about the standard operating procedure for that task type.</p>

        <h3>Spray Decomposition: A Closer Look</h3>

        <p>Spraying is the most complex decomposition, producing five sequential steps:</p>

        <ol>
          <li><strong>WAIT</strong> &mdash; Verify weather conditions are within safe limits for spraying (2 min)</li>
          <li><strong>CONFIGURE</strong> &mdash; Set up the robot: nozzle type, chemical, application rate, traversal pattern (3 min)</li>
          <li><strong>NAVIGATE</strong> &mdash; Travel from current location to the target field (5 min)</li>
          <li><strong>SPRAY</strong> &mdash; Execute the spraying pattern across the field (~15 min/hectare)</li>
          <li><strong>RETURN_BASE</strong> &mdash; Drive back to the base station (5 min)</li>
        </ol>

        <p>Duration estimates are parameterized by field area:</p>

<pre><code>area = _field_area(field_name, env)
spray_duration = max(10.0, area * 15.0)  # ~15 min per hectare</code></pre>

        <p>A 5-hectare field gets a spray step of 75 minutes; a 0.5-hectare plot is clamped to the 10-minute minimum. These estimates are rough but useful — they let the constraint engine check whether the plan fits within a deadline and give the farmer a meaningful time estimate in the plan explanation.</p>

        <p>The CONFIGURE step automatically defaults to a boustrophedon (back-and-forth) traversal pattern if the user didn't specify one. That's the standard approach for maximizing coverage efficiency in rectangular fields.</p>

        <h3>Weed Decomposition: Survey-Then-Act</h3>

        <p>The weeding pipeline adds an intermediate survey step:</p>

        <ol>
          <li><strong>NAVIGATE</strong> to the field</li>
          <li><strong>SURVEY</strong> for weed locations (mode: <code>weed_detection</code>)</li>
          <li><strong>WEED</strong> using mechanical removal</li>
          <li><strong>RETURN_BASE</strong></li>
        </ol>

        <p>This is a real pattern in precision agriculture. You survey first to build a weed map, then execute targeted removal rather than treating the entire field. CropCommand models this two-phase approach directly in the decomposition.</p>

        <h2>Walking Through the Example</h2>

        <p>Let's trace the complete example: <strong>"Spray the northern tomato field before the rain, avoiding the organic section."</strong></p>

        <h3>Step 1: Intent Detection</h3>

        <p>The word "spray" matches the <code>SPRAY</code> pattern. Confidence: 0.85.</p>

        <h3>Step 2: Entity Extraction</h3>

        <ul>
          <li><strong>Field references:</strong> "northern" matches the field pattern, producing <code>"Field North"</code>.</li>
          <li><strong>Crop references:</strong> "tomato" matches the crop vocabulary, added to <code>parameters["crops"]</code>.</li>
          <li><strong>Temporal constraints:</strong> "before the rain" — the word "before" triggers the deadline pattern. While "the rain" isn't a clock time, the temporal extractor captures the relative time signal. The constraint engine will cross-reference this with the weather forecast.</li>
          <li><strong>Spatial constraints:</strong> "avoiding the organic section" triggers the avoid pattern, producing <code>{"avoid_zones": ["organic"]}</code>.</li>
        </ul>

        <h3>Step 3: The ParsedCommand</h3>

<pre><code>ParsedCommand(
    intent=CommandIntent.SPRAY,
    target_field="Field North",
    robot_preference=None,
    temporal_constraint={"relative_time": "before the rain"},
    spatial_constraint={"avoid_zones": ["organic"]},
    parameters={"crops": ["tomato"]},
    raw_command="Spray the northern tomato field before the rain, avoiding the organic section",
    confidence=0.85,
)</code></pre>

        <h3>Step 4: Task Decomposition</h3>

        <p>The <code>_decompose_spray</code> function generates five actions. The avoid zones from the spatial constraint are injected into each task's parameters by the <code>PlanGenerator</code>:</p>

<pre><code>avoid_zones = parsed.spatial_constraint.get("avoid_zones", [])
if avoid_zones:
    for task in tasks:
        task.parameters["avoid_zones"] = avoid_zones</code></pre>

        <p>This means every step in the plan knows about the organic section restriction. The navigation planner routes around it; the spray execution skips it; the return path avoids it.</p>

        <h3>Step 5: The Execution Plan</h3>

        <p>The <code>PlanGenerator</code> wraps the tasks into an <code>ExecutionPlan</code> with a unique ID, a confidence score, and a human-readable explanation:</p>

<pre><code>Intent: spray. Target field: Field North.
Plan consists of 5 actions.
Temporal constraints: {'relative_time': 'before the rain'}.
Spatial constraints: {'avoid_zones': ['organic']}.</code></pre>

        <p>This plan isn't validated yet. It still needs to pass through the constraint engine (Part 2) and the human feedback loop (Part 3) before any robot moves.</p>

        <h2>Design Decisions and Trade-offs</h2>

        <p><strong>Why rule-based parsing instead of an LLM?</strong> Determinism and latency. In a safety-critical agricultural context, you need to know exactly how a command will be interpreted. Regex patterns are fast, predictable, and auditable. They can be extended by adding new patterns without retraining a model. The trade-off is coverage: unusual phrasings might not match. The low-confidence fallback mitigates this by flagging uncertain interpretations for human review.</p>

        <p><strong>Why Pydantic models?</strong> Validation at the boundary. Every <code>ParsedCommand</code>, <code>TaskAction</code>, and <code>ExecutionPlan</code> is a Pydantic <code>BaseModel</code> with type annotations, default values, and constraints (e.g., <code>confidence</code> is clamped to [0, 1]). Malformed data gets caught immediately rather than propagating through the pipeline and causing cryptic failures downstream.</p>

        <p><strong>Why a dispatch table for decomposition?</strong> Extensibility. Adding a new task type (say, <code>SEED</code> for planting operations) requires three changes: a new <code>CommandIntent</code> variant, a new <code>_decompose_seed</code> function, and a one-line entry in <code>_DECOMPOSITION_MAP</code>. No existing code is touched.</p>

        <h2>Key Takeaways</h2>

        <ul>
          <li><strong>Natural language is the natural interface for farm operators.</strong> Removing the translation step between human intent and robot action makes precision agriculture more accessible and more responsive to time-sensitive conditions.</li>
          <li><strong>Intent parsing and entity extraction are separable concerns.</strong> The intent answers "what"; the entities answer "where, when, how." Keeping them modular makes each component independently testable and extensible.</li>
          <li><strong>Task decomposition encodes domain expertise.</strong> The spray pipeline (weather check, configure, navigate, spray, return) reflects real agricultural standard operating procedures. The system's value comes from embedding that domain knowledge into structured, dependency-aware action sequences.</li>
          <li><strong>Confidence scores propagate uncertainty.</strong> When the parser isn't sure, that uncertainty follows the plan all the way to the user, enabling informed human oversight rather than blind trust.</li>
        </ul>

        <p>The next post will explore how constraint reasoning transforms a naive plan into one that respects the physical realities of weather, terrain, and robot capabilities.</p>

      </div>
      <div class="post-nav">
        <a href="../../blogs.html">&larr; Back to Blog</a>
        <a href="part2-constraints.html">Constraint Reasoning and Simulation &rarr;</a>
      </div>
    </article>
  </main>
  <footer class="footer">
    <div class="container">
      <p>&copy; 2026 Munyaradzi Comfort Zhou. Built with intention.</p>
    </div>
  </footer>
  <script src="../../js/main.js"></script>
</body>
</html>
